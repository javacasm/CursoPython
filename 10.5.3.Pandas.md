### Trabajando con ficheros de datos: Pandas

![](./images/pandas_logo.png)

**Pandas** en un paquete python creado por [Wes McKinney](https://wesmckinney.com/) pensado para importar, procesar y exportar grandes volúmenes de datos. Su simpático nombre viene de **pan**el **da**ta. Es un módulo Open Source y cualquiera puede añadir funcionalidad o corregir algún error visitando su [repositorio en github](https://github.com/pandas-dev/pandas).

Los datos con los que vamos a trabajar suelen estar almacenados en ficheros de diferentes formatos como CSV, Excel, HTML, bases de datos, JSON, etc. Otras veces estos conjuntos de datos estarán colgados en internet y necesitamos recuperarlos. Además estos ficheros suelen ser de gran tamaño y necesitamos procesarlos de manera eficiente.  Estos conjuntos de datos suelen estar estructurados en tablas y tras  importarlos podremos realizar operaciones de filtrado, agregación, consulta, ...

Para lidiar con todo esto utilizaremos el módulo **Pandas**, pensado para importar y trabajar con grandes tablas de datos. Es una de las herramientas más usadas hoy en día para *Minería de Datos*. Nos permite:

* Importar datos desde multitud de formatos. 
* Al importarlo sabe reconocer el formato de los datos (si son fechas o enteros o decimales, ...)
* Exportar nuestros datos a  multitud de formatos. 
* Trabajar eficientemente con enormes volúmenes de datos
* Revisar los datos
* Manipular los datos
* Seleccionar/filtrar un conjunto de los datos, bien unas columnas o unas filas o un subconjunto de los datos según sus valores.
* Generar nuevos datos a partir de las actuales
* Obtener agregados (medias, medianas y otros descriptores)
* Cambiar la estructura de los datos
* Combinar varias fuentes de datos.
* Trabajar con valores perdidos o incorrectos.

## Instalación

Si no tenemos instalado pandas (Google Colab lo trae instalado) instalamos el módulo con pip:

```sh
pip3 install pandas
```

También podemos instalar desde el gestor de paquetes de nuestro editor Thonny.

## Creación de DataFrame y acceso a los datos

Podemos usar datos almacenados en un diccionario

```python
dict = {"país": ["Brasil", "Rusia", "India", "China", "Sudáfrica"],
       "capital": ["Brasilia", "Moscú", "Nueva Dehli", "Pekín", "Pretoria"],
       "area": [8.516, 17.10, 3.286, 9.597, 1.221],
       "población": [200.4, 143.5, 1252, 1357, 52.98] }
datos = pd.DataFrame(dict)
print(datos)
```

Por defecto se usa un índice numérico para los datos, pero podemos usar unas etiquetas o claves diferentes añadiendo una colección a la propiedad "index" del DataFrame

```python
datos.index= ["BR","RU","IN","CH","SA"]
datos
```

Si en algún momento queremos eliminar las etiquetas y volver al índice numérico podemos hacerlo llamando al método *reset_index()*

Podemos acceder a los datos por columnas, usando el nombre de una de ellas

```python
datos["país"]
```

También podemos usar una columna como índice con el método *set_index()*

```python
datos.set_index('país')
```

Dentro de una columna accedemos a un elemento concreto por el índice o por su posición

```python
print(datos["país"]["RU"])
print(datos["país"][3])
```

También podemos acceder a un conjunto de columnas, pasando al DataFrame una lista de nombres de columnas

```python
datos[["país", "capital"]]
```

El resultado será un DataFrame que Colab nos permite explorar y filtra  con una herramienta específica.

Si queremos explorar una columna usando la misma herramienta podemos usar una lista únicamente con el nombre de la columna deseada

```python
datos[["capital"]]
```

También podemos acceder a un grupo de filas de datos especificando el rango que queremos recuperar:

```python
datos[0:3]
```

También podemos acceder a una fila concreta bien por posición usando *iloc* o por la clave de la fila usando *loc*

```python
print(datos.iloc[3])
print()
print(datos.loc["RU"])
```

## Importando datos desde ficheros

Vamos a subir un fichero a nuestro entor para acceder desde Colab.

Una vez subido podemos visualizarlo con la herramienta de datos haciendo clic sobre el mismo

Para recuperar los datos desde ficheros usaremos el método "read" según sea el formato del fichero.

Por ejemplo para recuperar un fichero csv haremos:

```python
import os
fichero_datos = "company_sales_data.csv"
if fichero_datos in os.listdir():
  pd.read_csv(fichero_datos)
```

Que nos devolverá un DataFrame con los datos contenidos en el fichero

También podemos recuperar datos desde Google Drive, sin más que conectar con nuestro almacenamiento

Dado que la ruta hasta los ficheros de Google Drive es algo más compleja, podemos buscar el fichero que queremos importar en la pestaña de los ficheros y "Copiar ruta"

```python
datos_csv=pd.read_csv("/content/drive/MyDrive/1_Datos/company_sales_data.csv")
datos_excel=pd.read_excel("/content/drive/MyDrive/1_Datos/company_sales_data.xlsx")
print(datos_excel)
```

También podemos recuperar ficheros que estén almacenados online, para lo que necesitaremos la URL el fichero

```python
import requests
url_fichero = "https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv"
nombre_fichero = "nba_all_elo.csv"

response = requests.get(url_fichero) # accedemos a la URL
response.raise_for_status()  # comprueba si se ha descargado bien

with open(nombre_fichero,"wb") as f:  # Guardamos la respuesta como fichero
  f.write(response.content)
print('Terminado!')
```

Usamos read_csv porque es el formato del fichero, también podíamos haber usado *read_html* o *read_xml* si el fichero tuviera ese formato

A partir de este fichero que hemos descargado ya podemos seguir trabajando

```python
nba_data = pd.read_csv(nombre_fichero)  # Usamos read_csv porque es el formato del fichero
```

## Exploración de datos

Vamos a explorar estos datos. Comenzamos viendo su estructura con *len* para ver el número de filas y con *shape* para ver el número de filas y columnas, es lo que conocemos como su **dimensionalidad**

```python
len(nba_data) # número de filas
nba_data.shape # número de filas y de columnas
```

Para explorar los datos vamos a ver las primeras filas con *head()*

```python
nba_data.head()
```

O acceder a las últimas filas con *tail()*

```python
nba_data.tail()
```

Si imprimimos con *print* un DataFrame con muchos datos nos mostrará un resumen con las primeras y últimas líneas además de la estructura de los datos indicando el número total de filas y columnas que contiene.

```python
print(nba_data)
```

Podemos obtener información relevante sobre el DataFrame de nuestro dados, por ejemplo es el tipo de dato que se utiliza para cada columna, o la memoria que consume usando el método *info()*

```python
nba_data.info()
```
Donde vemos el tipo empleado en cada columna. Pandas establece este tipo entre los valores que encuentra en la columna, asignando el tipo *object* cuando no reconoce un tipo más concreto.

Podemos obtener una descripción estadísticas de los valores en cada columna usando el método *describe()*

```python
nba_data.describe()
```

Vemos que sólo se muestran las columnas numéricas. Podemos incluir a las de tipo no numérico al llamar al método así: *describe(include=object)*

nba_data.describe(include=object)

Donde ahora nos incluye información como aquellos más repetidos o el número de valores únicos. Por ejemplo vemos que sólo tenemos 104 valores para el "team_id" y solo 53 para "fran_id".

Vamos a explorar un poco más nuestros datos, para ello vamos a ver las veces que se repiten los valores con el método *value_counts()*

```python
nba_data["team_id"].value_counts()

nba_data["fran_id"].value_counts()
```

Y aquí vemos algo raro: el equipo con id LAL (Los Angeles Lakers) aparecen 5078 pero "Lakers" aparece 6024, lo que nos dice que algún otro equipo se ha llamado "Lakers" antes...

## Filtrado de datos

Para desentrañar este misterio vamos a filtrar los datos de manera que seleccionemos la filas que tienen "Lakers" en la columna "fran_id". 
Lo hacemos usando un filtro llamando al método *loc* indicando la columna y el valor esperado: nba_data.loc

```python
nba_data.loc[nba_data["fran_id"] == "Lakers"]
```

Como lo que queremos es ver los distintos valores de la columna *fran_id* y contar su frecuencia llamaremos al método *value_counts()* indicando el nombre de la columna que queremos utilizar al llamar a *loc*

```python
nba_data.loc[nba_data["fran_id"] == "Lakers","team_id"].value_counts()
```

Podemos visualizar los registros de este equipo "MNL" filtrando:

```python
nba_data.loc[nba_data["team_id"]=="MNL"]
```

En muchas ocasiones nos interesa añadir columnas con datos elaborados a partir de los existentes.

En el caso de los datos de la nba vemos que hay una columna llamada "date_game" que a primera vista parece que tiene fechas pero que Pandas no ha interpretado como tal. Por ello vamos a crear una nueva columna "date_played" que tendrá formato de fecha (datetime) y que contendrá el valor de la columna "date_game" convertido a una fecha válida.

Para ello asignamos a esta nueva columna el valor de la otra columna convertida a fecha por el método "to_datetime(" de pandas:

```python
nba_data["date_played"] = pd.to_datetime(nba_data["date_game"])
```

Si ahora preguntamos la estructura a nuestro DataFrame, veremos que tiene 24 columnas y la nueva es de tipo datetime

```python
nba_data.info()
nba_data.head()
```

Con esta nueva columna vamos a ver entre qué fechas compitieron los MNL Lakers viendo los valores mínimo y máximo de las fechas de competición, aplicando los métodos *max()* y *min()* a las fechas en las que ha jugado el equipo

```python
print(nba_data.loc[nba_data["team_id"]=="MNL","date_played"].min())
print(nba_data.loc[nba_data["team_id"]=="MNL","date_played"].max())
```

Del mismo modo que hemos aplicado los métodos para ver el ḿinimo y el máxima de una columna, podemos aplicar funciones de agregado como *sum()* para ver la suma de los valores de una columna en un conjunto de filas o calcular el valor medio

```python
pts_bos=nba_data.loc[nba_data["team_id"]=="BOS","pts"] # creamos la variable para usarlo más fácilmente
print(f'min:{pts_bos.min()}')
print(f'med:{int(pts_bos.sum()/pts_bos.count())}')
print(f'max:{pts_bos.max()}')
print(f'sum:{pts_bos.sum()}')
```

También podemos obtener esa información como ya vimos con *describe()*

```python
pts_bos.describe()
```

Ya hemos visto que podemos filtrar por un valor concreto de una columna, también podemos hacerlo para ver valores que cumplen una determinada condición. Por ejemplo:

```python
nba_data[nba_data["pts"]>150 ] # Partidos donde se marcaron más de 150 puntos
```

## Selección de columnas y filtrado de datos

O si sólo nos interesa el nombre de los oponentes, tanteos y la fecha

```python
nba_data.loc[nba_data["pts"]>150 ,["team_id","pts","opp_id","opp_pts","date_played"]]
```

O también podemos preguntar si un valor es no nulo

```python
nba_data[nba_data["notes"].notnull()]
```

También podemos combinar varios filtros con los operadores lógicos correspondientes

```python
nba_data.loc[((nba_data["pts"]>150) | (nba_data["opp_pts"]>150)) & (nba_data["_iscopy"]== 0) ,["team_id","pts","opp_id","opp_pts","date_played"]]
```

Donde hemos incluído paréntesis para dejar claras las operaciones lógicas

## Agrupación de datos

Vamos a agrupar filas según los valores de alguna columna y a sumar, contar o promediar los valores de otras

```python
fran_group = nba_data.groupby(["fran_id","year_id"])  # agrupamos por equipo y por el año
print(fran_group["pts"].sum())   # obtenemos el total de puntos por equipo y año
```

## Transformando un DataFrame

Hemos visto que podemos modificar los datos de un DataFrame añadiendo columnas o agrupando, pero antes de empezar a borrar datos o a eliminar columnas, algo muy frecuente cuando estemos limpiando nuestros datos (data cleaning) conviene saber que podemos hacer una copia de los datos originales y trabajar con la copia, para así evitar pérdidas.

Lo haremos con el método *copy()* que nos devolverá una copia del DataFrame original

```python
df = nba_data.copy()
```

Sobre la que podemos definir nuevas columnas

```python
df["diferencia"]=df.pts - df.opp_pts
df.diferencia.max()
```

Donde hemos utilizado el nombre de la columna como propiedad del DataFrame

También podemos renombrar columnas con el método *rename(...)* al que le pasaremos un diccionario con los cambios de nombres a realizar.

Pandas está pensado para preservar los datos originales, por ello lo que cambiamos no se hace sobre el DataFrame original sino que nos devuelve un DataFrame modificado (gastamos más memoria para mantener los datos pero ganamos en seguridad ante posibles errores)

```python
df.info()
df_renamed=df.rename(columns={"game_result":"result","game_location":"location"}) # DataFrame modificado
df_renamed.info()
```

También podemos borrar columnas con el método "drop(...)". Por defecto el método borra los valores de esas columnas, si queremos borrar la columna debemos de incluir *axis = 1* al llamar al método

```python
print(df.shape)  # estructura inicial
df_borradas = df.drop(["elo_i","elo_n","opp_elo_i","opp_elo_n"],axis=1)
print(df.shape)  # los datos originales mantienen la forma
print(df_borradas.shape)  # dataframe sin las columnas borradas
```

Si en algún momento queremos hacer los cambios sobre el DataFrame original podemos hacerlo indicando el argumento *inplace=True*

## Limpieza de datos

Un paso necesario cuando trabajamos con grandes cantidades de datos es revisarlos y eliminar aquellos no están completos o no nos son útiles, es la fase de **limpieza de datos** (data cleaning)

Ya hemos visto que el método *info()* nos da información sobre el número de valores no nulos de las columnas

```python
df.info()
```

En nuestros datos de la nba vemos que la columna "notes" sólo tiene 5424 valores no nulos en las más de 125000 filas.

Podemos crear un Dataframe con las filas que contienen datos no-nulos usando el método *dropna()*

```python
df_with_notes=df.dropna()
df_with_notes.shape
```

Otra opción si la columna con valores nulos no es relevante para nuestro análisis es eliminarla usando el mismo método pero con el argumento "axis=1"

```
df_without_notes=df.dropna(axis=1)
df_without_notes.shape
```

También podemos rellenar con un valor por defecto los nulos usando el método *fillna(value = valor_por_defecto)* y usaremos "inplace=True" para hacer los cambios en el mismo DataFrame y no crear otra copia

```python
df_default_notes=df.copy()
df_default_notes["notes"].fillna(value="sin notas", inplace=True)
df_default_notes["notes"].describe()
```

También tenemos que revisar nuestros datos en busca de **valores inválidos** o fuera de rango, para lo que nos ayuda el método *describe()*

```python
df.describe()
```

Donde vemos por ejemplo que el rango de años es correcto, entre 1947 y 2015, pero que la columna "pts" tiene un valor mínimo de 0 lo que parece incorrecto.
Si vemos esa fila:

```python
df[df["pts"]==0]
```

Podemos filtrar esa fila quedándonos con las restantes

```python
df_valid=df[df.pts!=0]
df_valid.describe()
```

A medida que vamos conociendo los datos y revisándolos veremos condiciones que estos deben cumplir y podemos aplicar **reglas de consistencia**.

Por ejemplo vemos que la columna "game_result" tiene los valores "L" (lost, pierde) y "W" (win, gana) y podemos comprobar que esos valores se asignan correctamente en función del tanteo. 

Para ello haremos un filtro para valores que no cumplen esta condición y comprobaremos que el DataFrame resultante está vación con la propiedad *empty*


```python
df["game_result"].value_counts()

df[(df.pts > df.opp_pts) & (df.game_result != "W")].empty & df[(df.pts < df.opp_pts) & (df.game_result != "L")].empty
```

Si viéramos que existen filas inconsistentes las retiraríamos de nuestro DataFrame

## Combinando fuentes de datos

Vamos a ver ahora cómo podemos combinar distintas fuentes de datos, básicamente  añadiendo filas y columnas

```python
datos
datos_nuevos=pd.DataFrame({"país": ["Holanda","Japón","España"],"capital":["Amsterdam","Tokyo","Madrid"],"empleados":[10,30,32]})
datos_juntos=pd.concat([datos,datos_nuevos])
datos_juntos
```

## Referencias

[Pandas, Getting Started](https://pandas.pydata.org/docs/getting_started/index.html)

[Pandas, User Guide](https://pandas.pydata.org/docs/user_guide/index.html)
